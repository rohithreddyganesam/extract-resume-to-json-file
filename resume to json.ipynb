{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7967827f-aa73-40ad-90b2-3c35a2f11278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.7-cp311-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\grohi\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\grohi\\anaconda3\\lib\\site-packages (from python-docx) (4.9.0)\n",
      "Collecting PyMuPDFb==1.24.6 (from pymupdf)\n",
      "  Downloading PyMuPDFb-1.24.6-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/244.3 kB 960.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 235.5/244.3 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 244.3/244.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading PyMuPDF-1.24.7-cp311-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.5/3.2 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.1/3.2 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading PyMuPDFb-1.24.6-py3-none-win_amd64.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.2/12.5 MB 24.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.5/12.5 MB 22.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.5/12.5 MB 22.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.6/12.5 MB 22.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.5 MB 23.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.7/12.5 MB 23.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.6/12.5 MB 23.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.5/12.5 MB 23.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.5/12.5 MB 23.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.4/12.5 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.5 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.5 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 20.4 MB/s eta 0:00:00\n",
      "Installing collected packages: python-docx, PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.6 pymupdf-1.24.7 python-docx-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319b49b7-8981-4659-8d0f-7848ab111128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Data:\n",
      "{\n",
      "    \"name\": \"Ganesam Rohith Reddy\",\n",
      "    \"experience\": [\n",
      "        \"NUS (The National University of Singapore)\\tSingapore\",\n",
      "        \"Data science and Deep learning\",\n",
      "        \"Data Science Expertise: data science initiatives at NUS, employing advanced analytics techniques to extract actionable insights from complex datasets.\",\n",
      "        \"Deep Learning Innovation: deep learning research, developing state-of-the-art models for applications such as image recognition and natural language processing, contributing to academic advancements and practical solutions.\",\n",
      "        \"Cross-disciplinary Collaboration: Collaborating closely with interdisciplinary teams, bridging the gap between data science research and real-world applications, fostering innovation and impact across academic and industrial domains.\"\n",
      "    ],\n",
      "    \"education\": [\n",
      "        \"Bennett University (GPA: GPA:7.5 /10)\\tAug '21 \\u2014 May '25\",\n",
      "        \"\"\n",
      "    ],\n",
      "    \"projects\": [\n",
      "        \"Seed Quality Finder using CNN model Link\",\n",
      "        \"Developed a robust Convolutional Neural Network (CNN) model trained on a large dataset of seed images to accurately classify seeds based on quality parameters such as Bad, Good, Excellent, and Worst seed\",\n",
      "        \"Utilized transfer learning with CNN architectures to leverage their feature extraction capabilities, fine-tuning the model by adjusting the CNN layers. Applied techniques like data augmentation to enhance generalization and builds my own model by change the CNN layers.\",\n",
      "        \"Created an interactive web application using the Streamlit framework to provide a user-friendly interface for uploading seed images, visualizing classification results, and accessing additional features.\",\n",
      "        \"Football and NBA match prediction using ML model Link\",\n",
      "        \"Developed ML models using logistic regression, random forest, or gradient boosting on curated historical data. Applied feature selection, hyperparameter tuning, cross-validation, time-series analysis, and ensemble learning for accurate predictions.\",\n",
      "        \"Engineered a diverse feature set from match statistics, player performance data, and team rankings. Utilized techniques like player embeddings and temporal features for enhanced prediction accuracy in Football and NBA outcomes.\",\n",
      "        \"Implemented web scraping algorithms with Python libraries like BeautifulSoup and Scrapy to extract structured data from HTML pages. Employed XPath or CSS selectors for precise data extraction from sports websites, integrating with APIs for real-time updates.\",\n",
      "        \"Value for money using ML model Link\",\n",
      "        \"Utilized time series analysis and feature engineering on Amazon price data. Employed forecasting models like ARIMA or LSTM to predict future price movements, aiding optimal purchase timing for consumers\",\n",
      "        \"Conducted feature engineering on Amazon price data, incorporating product attributes, temporal features, and contextual factors. Employed feature scaling, one-hot encoding, and time-series decomposition techniques to enhance model training efficacy\",\n",
      "        \"Using ML for price drop prediction enables efficient budget management. Models forecast price movements, aiding consumers in strategic purchasing decisions, optimizing spending, and maximizing value from Amazon purchases through proactive budget allocation\",\n",
      "        \"SKILLS\",\n",
      "        \"Programming Languages\\tC++, Java, Python\",\n",
      "        \"Data Analysis\\tData Science, Analytics, Forecasting, Logistic Regression, Statistics\",\n",
      "        \"Machine Learning\\tDeep Learning, Algorithms, Gradient Boosting, Long Short-Term Memory (LSTM), Convolutional Neural Networks\",\n",
      "        \"Database Management\\tSQL (Programming Language), DBMS\",\n",
      "        \"Web Development\\tHTML, CSS, XPath\",\n",
      "        \"Deep Learning\\tCNN, ANN\",\n",
      "        \"\",\n",
      "        \"EXTRA-CURRICULARS\",\n",
      "        \"U-18 District level Football player and State level player\\tNellore India\",\n",
      "        \"College Level Football player\\tBennett, Noida\",\n",
      "        \"Collage Robotic club technical member head\\tBennett, Noida\",\n",
      "        \"\",\n",
      "        \"Collage Sports team photographer member head\\tBennett, Noida\"\n",
      "    ],\n",
      "    \"certifications\": [\n",
      "        \"\",\n",
      "        \"Machine Learning: Regression\",\n",
      "        \"Emily Fox\",\n",
      "        \"Specialized Models: Time Series and Survival Analysis\",\n",
      "        \"Mark J Grover\",\n",
      "        \"Predictive Modeling and Analytics\",\n",
      "        \"Dan Zhang\",\n",
      "        \"HTML, CSS, and JavaScript for Web Developers\",\n",
      "        \"Yaakov Chaikin\",\n",
      "        \"Data Analysis with Python\",\n",
      "        \"Joseph Santarcangelo\",\n",
      "        \"Databases and SQL for Data Science with Python\",\n",
      "        \"Rav Ahuja\",\n",
      "        \"\",\n",
      "        \"Oct '23\",\n",
      "        \"\",\n",
      "        \"Oct '23\",\n",
      "        \"\",\n",
      "        \"Apr '23\",\n",
      "        \"\",\n",
      "        \"Apr '23\",\n",
      "        \"\",\n",
      "        \"Dec '22\",\n",
      "        \"\",\n",
      "        \"Dec '22\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Parsed data saved to parsed_resume_data.json\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import json\n",
    "\n",
    "def parse_resume_docx(filename):\n",
    "    doc = Document(filename)\n",
    "    resume_data = {}\n",
    "    \n",
    "    # Initialize sections of interest\n",
    "    sections_of_interest = ['EXPERIENCE', 'EDUCATION', 'PROJECTS', 'CERTIFICATIONS']\n",
    "    \n",
    "    # Extract name\n",
    "    resume_data['name'] = doc.paragraphs[0].text if len(doc.paragraphs) > 0 else ''\n",
    "    \n",
    "    # Extract sections of interest\n",
    "    for section in sections_of_interest:\n",
    "        section_content = []\n",
    "        section_found = False  # Initialize section_found\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if section in paragraph.text.upper():\n",
    "                section_found = True\n",
    "                continue\n",
    "            if section_found:\n",
    "                if any(ext in paragraph.text for ext in sections_of_interest):\n",
    "                    section_found = False\n",
    "                    break\n",
    "                section_content.append(paragraph.text.strip())\n",
    "        resume_data[section.lower()] = section_content\n",
    "\n",
    "    return resume_data\n",
    "\n",
    "def save_as_json(data, output_filename):\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filename = 'resume_ganesam_rohith_reddy.docx'  # Replace with your actual filename\n",
    "    parsed_data = parse_resume_docx(filename)\n",
    "    \n",
    "    # Print parsed data\n",
    "    print(\"Parsed Data:\")\n",
    "    print(json.dumps(parsed_data, indent=4))\n",
    "    print()\n",
    "    \n",
    "    output_filename = 'parsed_resume_data.json'  # Name your output JSON file\n",
    "    save_as_json(parsed_data, output_filename)\n",
    "    print(f'Parsed data saved to {output_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79042395-7b32-4d8a-af5a-8638cb422f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
